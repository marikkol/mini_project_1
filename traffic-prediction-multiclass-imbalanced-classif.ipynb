{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Traffic Prediction - Multi-Class Imbalanced Classification\n","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I will try to build a model that predicts the traffic situation in New York City, according to 3 levels: 0-Low,1-Medium ,2-High.\n\nUsing this model it will be possible to assess the future traffic situation within the city, based on a small number of features.\n\nGiven the 3 data sets, I will choose to use \"nyc_traffic_ml_orig\".","metadata":{}},{"cell_type":"markdown","source":"\n### A brief explanation of the variables in the table:\n\n- Segment_ID: is an identifier for each street segment.\n\n- Is_Weekend: tells whether it's a weekend or not.\n\n- Daylight: lighting percentage (the hours of the day placed in a bell-curving function f(x) = (2)/(1 + np.exp(((x - 12)/4)^2))).\n\n- Season1-4: One-Hot-Encoded Seasons (In this dataset, information about the summer is missing).\n\n- Hr3-24: One-Hot-Encoded hours of the day to a 3 hour interval.\n\n- Traffic_Volume: number of vehicles passing through a road over a period of hour (aggregated average for 3 hours).\n\n- StreetWidth: standardized street width.\n\n- UnitsRes_Prop:  percentage of residential units.\n\n- NumFloors: numbers of floors.\n\n- SubwayProximity: proximity to the Subway (1/NEAR_DIST).\n\n- BoroCode1-5: One-Hot-Encoded of the five boroughs of New York City (Bronx, Brooklyn, Manhattan, Queens and Staten Island).\n\n- LandUse_t1-5: environmental context (Residential, Commercial, Infrastructure, Public Institution, Open Space)","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Analysis\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.143123Z","iopub.execute_input":"2022-01-16T14:39:46.143755Z","iopub.status.idle":"2022-01-16T14:39:46.150631Z","shell.execute_reply.started":"2022-01-16T14:39:46.143712Z","shell.execute_reply":"2022-01-16T14:39:46.149169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load and check data\ndf = pd.read_csv(\"../input/nyc-traffic-data/nyc_traffic_ml_orig.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.187614Z","iopub.execute_input":"2022-01-16T14:39:46.188069Z","iopub.status.idle":"2022-01-16T14:39:46.359971Z","shell.execute_reply.started":"2022-01-16T14:39:46.188037Z","shell.execute_reply":"2022-01-16T14:39:46.359128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.361837Z","iopub.execute_input":"2022-01-16T14:39:46.36208Z","iopub.status.idle":"2022-01-16T14:39:46.378495Z","shell.execute_reply.started":"2022-01-16T14:39:46.36205Z","shell.execute_reply":"2022-01-16T14:39:46.377421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for null and missing values\ndf.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.380319Z","iopub.execute_input":"2022-01-16T14:39:46.380635Z","iopub.status.idle":"2022-01-16T14:39:46.391357Z","shell.execute_reply.started":"2022-01-16T14:39:46.380592Z","shell.execute_reply":"2022-01-16T14:39:46.390181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#descriptive statistics summary\ndf['Traffic_Volume'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.393897Z","iopub.execute_input":"2022-01-16T14:39:46.394573Z","iopub.status.idle":"2022-01-16T14:39:46.408214Z","shell.execute_reply.started":"2022-01-16T14:39:46.394524Z","shell.execute_reply":"2022-01-16T14:39:46.407404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distribution plot\nsns.distplot(df['Traffic_Volume'])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.40944Z","iopub.execute_input":"2022-01-16T14:39:46.410158Z","iopub.status.idle":"2022-01-16T14:39:46.874344Z","shell.execute_reply.started":"2022-01-16T14:39:46.410122Z","shell.execute_reply":"2022-01-16T14:39:46.873409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A distribution plot of the traffic volume shows the large number of samples belonging to a low traffic volume, and that there are too few examples for higher traffic volume.\n\nI will deal with the imbalance of our target values later.","metadata":{}},{"cell_type":"code","source":"#the correlation of the target variable\ndf = df[['Traffic_Volume']+[c for c in df.columns if c != 'Traffic_Volume']]\ncorrmat = df.corr()\ncorr_df = pd.DataFrame(corrmat.values[0,:], index=df.columns, columns=['target_corr'])\ncorr_df = corr_df.drop(['Traffic_Volume'])\ncorr_df","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.87595Z","iopub.execute_input":"2022-01-16T14:39:46.876252Z","iopub.status.idle":"2022-01-16T14:39:46.96781Z","shell.execute_reply.started":"2022-01-16T14:39:46.876212Z","shell.execute_reply":"2022-01-16T14:39:46.966703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are so many variables, I will take the variables with highst correlations with our target.","metadata":{}},{"cell_type":"code","source":"#choosing the highs correlation variables\nk = 15\ncols = corrmat.nlargest(k, 'Traffic_Volume')['Traffic_Volume'].index\ncols = [col for col in cols if col != 'Traffic_Volume']\ncols","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.969095Z","iopub.execute_input":"2022-01-16T14:39:46.969541Z","iopub.status.idle":"2022-01-16T14:39:46.979194Z","shell.execute_reply.started":"2022-01-16T14:39:46.969504Z","shell.execute_reply":"2022-01-16T14:39:46.978297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation matrix\ncorrmat_sel = df[['Traffic_Volume'] + list(cols)].corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat_sel, cbar=True, annot=True, square=True);","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:46.980792Z","iopub.execute_input":"2022-01-16T14:39:46.981284Z","iopub.status.idle":"2022-01-16T14:39:48.857715Z","shell.execute_reply.started":"2022-01-16T14:39:46.981222Z","shell.execute_reply":"2022-01-16T14:39:48.856734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that there are high correlations between the daytime hours and the Daylight feature (remember that Daylight is derived from them), so I will remove the daytime hours from the list.","metadata":{}},{"cell_type":"code","source":"cols = [col for col in cols if col not in ['Hr9','Hr12','Hr15','Hr18','Hr21']]\ncols","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:48.859038Z","iopub.execute_input":"2022-01-16T14:39:48.859429Z","iopub.status.idle":"2022-01-16T14:39:48.86603Z","shell.execute_reply.started":"2022-01-16T14:39:48.859396Z","shell.execute_reply":"2022-01-16T14:39:48.865186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now lets categorize the target Traffic_Volume:","metadata":{}},{"cell_type":"code","source":"#categorizing the target\nmax_value = df['Traffic_Volume'].max()\nsplit_1, split_2 = max_value/3, max_value*2/3\nprint(split_1, split_2)\ndf['Traffic_Volume'] = pd.cut(df['Traffic_Volume'], bins=[0,split_1,split_2,np.inf], include_lowest=True, labels=[0, 1, 2])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:48.868896Z","iopub.execute_input":"2022-01-16T14:39:48.869172Z","iopub.status.idle":"2022-01-16T14:39:48.884382Z","shell.execute_reply.started":"2022-01-16T14:39:48.869123Z","shell.execute_reply":"2022-01-16T14:39:48.883367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize distribution\ny = df['Traffic_Volume'].values\ncounter = Counter(y)\nfor k,v in counter.items():\n    per = v / len(y) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\nplt.bar(counter.keys(), counter.values())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:48.885976Z","iopub.execute_input":"2022-01-16T14:39:48.886444Z","iopub.status.idle":"2022-01-16T14:39:49.02716Z","shell.execute_reply.started":"2022-01-16T14:39:48.886406Z","shell.execute_reply":"2022-01-16T14:39:49.026259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that there is a problem of highly imbalanced classes, which can make our classifier to be more biased towards the majority class, causing bad classification of the minority class.\nTo deal with this problem I will use SMOTE Oversampling which synthesizing new examples of the minority classes so that the number of examples in the minority class matches the number of examples in the majority classes.","metadata":{}},{"cell_type":"markdown","source":"### Modeling","metadata":{}},{"cell_type":"code","source":"#train test splitting\nX = df[cols]\ny = df['Traffic_Volume']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:49.029175Z","iopub.execute_input":"2022-01-16T14:39:49.02991Z","iopub.status.idle":"2022-01-16T14:39:49.044808Z","shell.execute_reply.started":"2022-01-16T14:39:49.029857Z","shell.execute_reply":"2022-01-16T14:39:49.043842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform the dataset\noversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train)\nfor k,v in counter.items():\n    per = v / len(y_train) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n\nplt.bar(counter.keys(), counter.values())\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:49.046434Z","iopub.execute_input":"2022-01-16T14:39:49.046948Z","iopub.status.idle":"2022-01-16T14:39:49.292051Z","shell.execute_reply.started":"2022-01-16T14:39:49.046904Z","shell.execute_reply":"2022-01-16T14:39:49.291182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building a random forest classifier\nclf = RandomForestClassifier(n_estimators=1000, random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:39:49.29347Z","iopub.execute_input":"2022-01-16T14:39:49.293958Z","iopub.status.idle":"2022-01-16T14:40:40.678393Z","shell.execute_reply.started":"2022-01-16T14:39:49.293912Z","shell.execute_reply":"2022-01-16T14:40:40.677447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#evaluate a model\ndef evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\nscores = evaluate_model(X_train, y_train, clf)\nscores.mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:40:40.680142Z","iopub.execute_input":"2022-01-16T14:40:40.680465Z","iopub.status.idle":"2022-01-16T14:45:02.796907Z","shell.execute_reply.started":"2022-01-16T14:40:40.680422Z","shell.execute_reply":"2022-01-16T14:45:02.795856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I got an accuracy that looks pretty good, but keep in mind that for imbalanced data the accuracy metric is not a good metric for measuring the correctness of our model.\n\nTo get a better idea of how our model is performing I will use Confusion Matrix.","metadata":{}},{"cell_type":"code","source":"#Confusion Matrix\ncf_matrix = confusion_matrix(y_test, y_pred)\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n\" for v1, v2 in zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(3,3)\nax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nax.set_title('Confusion Matrix\\n');\nax.set_xlabel('\\nPredicted Traffic Category')\nax.set_ylabel('Actual Traffic Category ');\nax.xaxis.set_ticklabels(['0','1', '2'])\nax.yaxis.set_ticklabels(['0','1', '2'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:45:02.798771Z","iopub.execute_input":"2022-01-16T14:45:02.799364Z","iopub.status.idle":"2022-01-16T14:45:03.101156Z","shell.execute_reply.started":"2022-01-16T14:45:02.799318Z","shell.execute_reply":"2022-01-16T14:45:03.100195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the Confusion Matrix it can be seen that the results are not very good, and the model tends to classify the classes with the higher traffic to the lower class. But given the large imbalance in the given data (94.7% - 5% - 0.3%), the model still classifies most of the samples in each class correctly.\n\nProbably if we have more samples with high traffic volume (not just 77), our model will give better results and can provide us more reliable predictions.\n\n","metadata":{}}]}